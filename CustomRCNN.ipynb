{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "UzTa_kedWDCr"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import pycocotools\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
    "from torchvision.ops import MultiScaleRoIAlign,nms,box_convert,box_iou\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "k1I7nHUPCnb7"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageList:\n",
    "    \"\"\"\n",
    "    Structure that holds a list of images (of possibly\n",
    "    varying sizes) as a single tensor.\n",
    "    This works by padding the images to the same size,\n",
    "    and storing in a field the original sizes of each image\n",
    "\n",
    "    Args:\n",
    "        tensors (tensor): Tensor containing images.\n",
    "        image_sizes (list[tuple[int, int]]): List of Tuples each containing size of images.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tensors: Tensor, image_sizes: List[Tuple[int, int]]) -> None:\n",
    "        self.tensors = tensors\n",
    "        self.image_sizes = image_sizes\n",
    "\n",
    "    def to(self, device: torch.device) -> \"ImageList\":\n",
    "        cast_tensor = self.tensors.to(device)\n",
    "        return ImageList(cast_tensor, self.image_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_dimensions(dataset):\n",
    "    max_height = 0\n",
    "    max_width = 0\n",
    "\n",
    "    for image, annotation in dataset:\n",
    "        width, height = image.shape[2],image.shape[1]\n",
    "        max_height = max(max_height, height)\n",
    "        max_width = max(max_width, width)\n",
    "\n",
    "    return max_height, max_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_max_size(image, target_height, target_width):\n",
    "    width, height = image.shape[2],image.shape[1]\n",
    "    pad_height = target_height - height\n",
    "    pad_width = target_width - width\n",
    "\n",
    "    padding = (0, 0, pad_width, pad_height)\n",
    "    padded_image = F.pad(image, padding, value=0)\n",
    "    return padded_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PadToMaxSize:\n",
    "    def __init__(self, max_height, max_width):\n",
    "        self.max_height = max_height\n",
    "        self.max_width = max_width\n",
    "\n",
    "    def __call__(self, image):\n",
    "        padding_top = 0\n",
    "        padding_left = 0\n",
    "        padding_bottom = max(0, self.max_height - image.shape[1])\n",
    "        padding_right = max(0, self.max_width - image.shape[2])\n",
    "        \n",
    "        return F.pad(image, (padding_left, padding_right, padding_top, padding_bottom))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_mean_std(dataset):\n",
    "    mean = torch.zeros(3).to(device)\n",
    "    std = torch.zeros(3).to(device)\n",
    "    n_images = 0\n",
    "\n",
    "    for image,annotation in dataset:\n",
    "        mean += torch.mean(image, [1, 2])\n",
    "        std += torch.std(image, [1, 2])\n",
    "        n_images += 1\n",
    "\n",
    "    mean /= n_images\n",
    "    std /= n_images\n",
    "    return mean, std\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=51.73s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "dataset=CocoDetection(root='./content/train2017',annFile='./content/annotations/instances_train2017.json',transform=transforms.ToTensor())\n",
    "small_train_ds = torch.utils.data.Subset(dataset, range(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_height,max_width=find_max_dimensions(small_train_ds)\n",
    "pad_transform=transforms.Compose([PadToMaxSize(max_height,max_width)])\n",
    "small_train_ds.transform=pad_transform\n",
    "mean,std=compute_mean_std(small_train_ds)\n",
    "\n",
    "image_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean, std)])\n",
    "small_train_ds.transform=image_transform\n",
    "#normalize(small_train_ds,mean,std)\n",
    "small_train_loader=DataLoader(small_train_ds,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "J461yZ78Wy0C"
   },
   "outputs": [],
   "source": [
    "class RPN(nn.Module):\n",
    "  def __init__(self,in_channels,anchors):\n",
    "    super(RPN,self).__init__()\n",
    "\n",
    "    self.conv = nn.Conv2d(in_channels,512,kernel_size=3,stride=1,padding=1)\n",
    "    self.cls_logits = nn.Conv2d(512,anchors*2,kernel_size=1,stride=1)\n",
    "    self.bbox_pred = nn.Conv2d(512,anchors*4,kernel_size=1,stride=1)\n",
    "\n",
    "    for layer in [self.conv,self.cls_logits,self.bbox_pred]:\n",
    "      nn.init.normal_(layer.weight,std=0.01)\n",
    "      nn.init.constant_(layer.bias,0)\n",
    "\n",
    "    self.anchors = anchors\n",
    "\n",
    "  def forward(self,x):\n",
    "    logits = []\n",
    "    bbox_preds = []\n",
    "    for feature in x:\n",
    "      t = F.relu(self.conv(feature))\n",
    "      logits.append(self.cls_logits(t).permute(0, 2, 3, 1).reshape(t.shape[0], -1, 2))  # (N, H*W*K, 2)\n",
    "      bbox_preds.append(self.bbox_pred(t).permute(0, 2, 3, 1).reshape(t.shape[0], -1, 4))  # (N, H*W*K, 4)\n",
    "      #break\n",
    "    \n",
    "    logits = torch.cat(logits, dim=1)  # Shape: (total_anchors, 2)\n",
    "    bbox_preds = torch.cat(bbox_preds, dim=1)  # Shape: (total_anchors, 4)\n",
    "    \n",
    "\n",
    "\n",
    "    return logits,bbox_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "xbDXg23Yn8s6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((32, 64, 128), (64, 128, 256), (128, 256, 512))\n",
      "((0.5, 1.0, 2.0), (0.5, 1.0, 2.0), (0.5, 1.0, 2.0))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "anchor_generator=AnchorGenerator(sizes=((32,64,128),(64,128,256),(128,256,512)),\n",
    "                                   aspect_ratios=(0.5,1.0,2.0),)\n",
    "print(anchor_generator.sizes)\n",
    "print(anchor_generator.aspect_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "ZyzuKLkanp7D"
   },
   "outputs": [],
   "source": [
    "class DetectionHead(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, roi_output_size):\n",
    "        super(DetectionHead, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(in_channels * roi_output_size ** 2, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 1024)\n",
    "\n",
    "        self.cls_score = nn.Linear(1024, num_classes)\n",
    "        self.bbox_pred = nn.Linear(1024, num_classes * 4)\n",
    "\n",
    "        for layer in [self.fc1, self.fc2, self.cls_score, self.bbox_pred]:\n",
    "            torch.nn.init.normal_(layer.weight, std=0.01)\n",
    "            torch.nn.init.constant_(layer.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        cls_logits = self.cls_score(x)\n",
    "        bbox_deltas = self.bbox_pred(x)\n",
    "\n",
    "        return cls_logits, bbox_deltas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s12dhfVOsB6L"
   },
   "outputs": [],
   "source": [
    "\n",
    "class FasterRcnn(nn.Module):\n",
    "  def __init__(self,backbone,num_classes,anchor_generator):\n",
    "    super(FasterRcnn,self).__init__()\n",
    "\n",
    "    self.backbone = backbone\n",
    "    self.rpn = RPN(backbone.out_channels,anchors=9)\n",
    "    self.head = DetectionHead(in_channels=backbone.out_channels,num_classes=num_classes,roi_output_size=7)\n",
    "    self.roi_poooler= MultiScaleRoIAlign(featmap_names=['1','2','3'],output_size=7,sampling_ratio=2)\n",
    "    \n",
    "    self.num_classes = num_classes\n",
    "\n",
    "    self.anchor_generator = anchor_generator\n",
    "    self.classification_loss_fn=nn.CrossEntropyLoss()\n",
    "    self.regression_loss_fn=nn.SmoothL1Loss()\n",
    "\n",
    "  def gen_anchors(self, features, images):\n",
    "    #Initial feature shapes e o lista cu len de 3\n",
    "    feature_shapes = [feat[-2:] for feat in features] \n",
    "    #Dupa ce iese din anchor generator e o lista cu un len de 1 ce contine tensorii cu ancore de shape (N,4)\n",
    "    anchors = self.anchor_generator(images,feature_shapes)\n",
    "   \n",
    "    return anchors\n",
    "\n",
    "  def process_proposals(self,objectness_logits,bbox_deltas,anchors,pos_thresh=0.7,neg_thresh=0.3,num_proposals=1000):\n",
    "        \n",
    "        #We flatten the bbox deltas to match the anchors , and get the foreground (object) value from the clasification logits\n",
    "        print(bbox_deltas.shape)\n",
    "        bbox_deltas = bbox_deltas.view(-1, 4) # Shape (anchors,4)\n",
    "        objectness_logits=objectness_logits.view(-1,2)# Shape(anchors,2) - 2 reprezinta clasele din partea de clasifier de RPN (0-background,1-foreground)\n",
    "        logits_prob = torch.sigmoid(objectness_logits)[:, 1]# Am incercat sa filtrez peste clasele de foreground\n",
    "       \n",
    "        #Filter the proposals (applying bbox deltas to anchors)\n",
    "        anchors=torch.cat(anchors,dim=0)\n",
    "        proposals=box_convert(anchors,in_fmt='cxcywh',out_fmt='xyxy')\n",
    "        proposals=proposals+bbox_deltas\n",
    "\n",
    "        #We filter results by threshold\n",
    "        keep=logits_prob>0.5\n",
    "        proposals=proposals[keep]\n",
    "        logits_prob=logits_prob[keep]\n",
    "        keep = nms(proposals, logits_prob, neg_thresh)\n",
    "        keep = keep[:num_proposals]\n",
    "        proposals = proposals[keep]\n",
    "        objectness_logits = objectness_logits[keep]\n",
    "\n",
    "        return proposals\n",
    "  def filter_proposals(self,proposals,cls_logits,bbox_pred,gt_boxes,gt_labels):\n",
    "\n",
    "        print(len(proposals))\n",
    "        iou_matrix = box_iou(proposals, gt_boxes)  # Shape(Proposals,gt_boxes)\n",
    "        max_iou, matched_idxs = iou_matrix.max(dim=1) \n",
    "        \n",
    "        fg_iou_thresh=0.4\n",
    "        bg_iou_thresh=0.2\n",
    "        \n",
    "        labels = torch.full((proposals.size(0),), -1, dtype=torch.long, device=proposals.device)  # -1 <=>ignore\n",
    "        fg_mask = max_iou >= fg_iou_thresh\n",
    "        bg_mask = (max_iou < fg_iou_thresh) & (max_iou >= bg_iou_thresh)\n",
    "\n",
    "        if fg_mask.any():\n",
    "            labels[fg_mask] = gt_labels[matched_idxs[fg_mask]]\n",
    "            labels[bg_mask] = 0 #background\n",
    "\n",
    "\n",
    "        bbox_targets = torch.zeros_like(bbox_pred)\n",
    "        if fg_mask.any():\n",
    "            fg_proposals = proposals[fg_mask]\n",
    "            fg_gt_boxes = gt_boxes[matched_idxs[fg_mask]]\n",
    "            fg_gt_labels = gt_labels[matched_idxs[fg_mask]]\n",
    "            fg_bbox_targets = self.compute_bbox_targets(fg_proposals, fg_gt_boxes)\n",
    "\n",
    "            for idx, label in enumerate(fg_gt_labels):\n",
    "          \n",
    "              class_idx = label.item()  #class index\n",
    "              start = class_idx * 4\n",
    "              end = start + 4\n",
    "              bbox_targets[fg_mask][idx, start:end] = fg_bbox_targets[idx]\n",
    "\n",
    "\n",
    "        # Classification loss\n",
    "\n",
    "        valid_mask = labels >= 0\n",
    "        classification_loss=torch.tensor(0.0,requires_grad=True)\n",
    "        if valid_mask.any():\n",
    "          filtered_logits=cls_logits[valid_mask]\n",
    "          filtered_labels=labels[valid_mask]\n",
    "          classification_loss = self.classification_loss_fn(filtered_logits, filtered_labels)\n",
    "      \n",
    "        regression_loss =torch.tensor(0.0,requires_grad=True)\n",
    "        if fg_mask.any():\n",
    "            regression_loss = self.regression_loss_fn(bbox_pred[fg_mask], bbox_targets[fg_mask])\n",
    "            regression_loss /= fg_mask.sum().float()  \n",
    "\n",
    "        return classification_loss, regression_loss\n",
    "  \n",
    "  def compute_bbox_targets(self,proposals, gt_boxes):\n",
    "        \"\"\"\n",
    "        Computes regression targets (dx, dy, dw, dh) for proposals.\n",
    "        \"\"\"\n",
    "        proposals = box_convert(proposals, 'xyxy', 'cxcywh')\n",
    "       \n",
    "        gt_boxes = box_convert(gt_boxes, 'xyxy', 'cxcywh')\n",
    "        \n",
    "\n",
    "        targets_dx = (gt_boxes[:, 0] - proposals[:, 0]) / proposals[:, 2]\n",
    "        targets_dy = (gt_boxes[:, 1] - proposals[:, 1]) / proposals[:, 3]\n",
    "        targets_dw = torch.log(gt_boxes[:, 2] / proposals[:, 2])\n",
    "        targets_dh = torch.log(gt_boxes[:, 3] / proposals[:, 3])\n",
    "\n",
    "        return torch.stack((targets_dx, targets_dy, targets_dw, targets_dh), dim=1)\n",
    "\n",
    "\n",
    "  def forward(self,images,targets):\n",
    "      featmaps = self.backbone(images)\n",
    "      features=([featmaps[f'{i}'] for i in range(1,4)])#Filtare pentru a lua layerele 1,2,3 din featmap (fiecare layer are un scale diferit)\n",
    "      image_sizes = [(image.shape[1], image.shape[2]) for image in images]\n",
    "      image_list = ImageList(images, image_sizes)\n",
    "\n",
    "      \n",
    "      anchors=self.gen_anchors(features,image_list)#(1,N,4)\n",
    "      #print('trece de ancore')\n",
    "      \n",
    "      logits,bbox_deltas = self.rpn(features)#(1,N,2) si (1,N,4)\n",
    "      #print('trece de rpn')\n",
    "    \n",
    "      proposals=self.process_proposals(logits,bbox_deltas,anchors)\n",
    "      #print('trece de proposals')\n",
    "\n",
    "      #MultiRoiScaleAlign <=> RoiPooler aici necesita ca parametri featmaps(sunt filtrate din declararea la roi_pooler),Lista de tensori pentru boxes (cu toate ca la mine e doar un tensor cu shape (N,4),lista de image_size)\n",
    "      #N fiind numar de ancore\n",
    "      pooled_features=self.roi_poooler(featmaps,[proposals],image_list.image_sizes)\n",
    "      #print('trece de roi')\n",
    "\n",
    "      cls_logits,bbox_pred = self.head(pooled_features)\n",
    "      # cls_logits: (Proposals, num_classes) -> (1, Proposals, num_classes)\n",
    "      cls_logits = cls_logits.unsqueeze(0)\n",
    "\n",
    "      # bbox_pred: (Proposals, num_classes * 4) -> (1, Proposals, 4, num_classes)\n",
    "      num_classes = bbox_pred.shape[1] // 4 \n",
    "      bbox_pred = bbox_pred.view(-1, 4, num_classes).unsqueeze(0)  \n",
    "\n",
    "\n",
    "      #Ar trebui sa dau reshape in (Batch_size,N,...)\n",
    "\n",
    "      #gt_boxes = torch.cat([torch.tensor(t['bbox'], device=device).view(1, 4) for t in targets], dim=0)\n",
    "      #gt_labels = torch.cat([torch.tensor(t['category_id'], device=device) for t in targets], dim=0)\n",
    "\n",
    "      #cls_loss,regr_loss=self.filter_proposals(proposals,cls_logits,bbox_pred,gt_boxes,gt_labels)\n",
    "\n",
    "      gt_boxes = torch.tensor([t['bbox'] for t in targets], device=device).view(-1,len(targets), 4)\n",
    "\n",
    "      gt_labels = torch.tensor([t['category_id'] for t in targets], device=device).view(-1,len(targets))\n",
    "\n",
    "      num_gt_boxes = gt_labels.shape[1]\n",
    "      fg_probs = cls_logits[0, :, 1:].max(dim=-1)[0]\n",
    "      top_indices = torch.topk(fg_probs, num_gt_boxes, dim=0).indices\n",
    "      cls_logits = cls_logits[:, top_indices, :]\n",
    "      bbox_pred = bbox_pred[:, top_indices, :, :]\n",
    "      print(cls_logits.shape)\n",
    "      print(bbox_pred.shape)\n",
    "\n",
    "      cls_loss = self.classification_loss_fn(\n",
    "        cls_logits.view(-1, num_classes),  #(1, N, num_classes) -> (N, num_classes)\n",
    "        gt_labels.view(-1)  #(1, N) -> (N,)\n",
    "    )\n",
    "      reg_loss_per_class = []\n",
    "      for class_idx in range(1, num_classes):  #fara clasa 0 (background)\n",
    "        pred_bboxes_class = bbox_pred[0, :, :, class_idx]  # (N, 4)\n",
    "\n",
    "        gt_bboxes_class = gt_boxes[0]  # (N, 4)\n",
    "\n",
    "        reg_loss = self.regression_loss_fn(pred_bboxes_class, gt_bboxes_class)\n",
    "        reg_loss_per_class.append(reg_loss)\n",
    "\n",
    "      regr_loss = torch.mean(torch.stack(reg_loss_per_class))\n",
    "\n",
    "      return cls_loss,regr_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "eKp3ltBetNPM"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def train(model, train_loader, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time() \n",
    "        \n",
    "        running_loss = 0.0\n",
    "        for images, targets in train_loader:\n",
    "            images = images.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            classification_loss, bbox_regression_loss = model(images, targets)\n",
    "            loss = classification_loss + bbox_regression_loss\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_time = time.time() - start_time  \n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}, Time: {epoch_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=FasterRcnn(backbone=resnet_fpn_backbone('resnet50',weights=ResNet50_Weights.DEFAULT),num_classes=91,anchor_generator=anchor_generator)\n",
    "optimizer=optim.Adam(model.parameters(),lr=0.001)\n",
    "train(model,small_train_loader,optimizer,num_epochs=1000)\n",
    " "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
